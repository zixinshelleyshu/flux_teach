# LSTM in Flux Tutorial {#ch-14}

### Motivation

XXX add the following contents

-   Explain independence assumption, and that feed-forward DNN (as well as many other widely used ML algorithms) are time-agnostic ("sequence-agnostic").

-   Explain that some data has temporal dependencies by nature and modelling for these data requires ML that can learn them.

-   Explain that we can also combine time-invariant data with temporal data in "special" model architectures.

-   Introduce example data

XXX since we have only one site, it doesn't make sense to include the meta data into model training. Respective variables are essentially zero-variance variables! Let's keep this for a next chapter and only treat the time-dynamic variables here.

### LSTM

-   Explain the functioning of an LSTM cell referring to the blog post

-   Explain why and how LSTMs can be combined with fully connected layers a DNN

XXX General

-   The dataframe `df` now contains no date information. I know, this is not used in the training, but should be kept for visualisation purposes. We have to show that the data is a sequence over dates.
-   Use a different site, e.g., CH-Dav or CH-Lae

### Example of LSTM+MLP on Flux data

We will start with importing the needed packages.

```{r install packages,warning=F, message=F}
library(torch)
library(luz)
library(tidytable)
library(tidyverse)
library(skimr)
library(recipes)
```

Next, we import the dataset which including the daliy measurements of one site and glimpse the first row.

```{r import data}
df  <-  read_csv("data/df.csv")
skim(df)
```

XXX keep temporal data and site meta info in separate files and handle data frames separately

```{r}
meta_vars <- c("classid", "koeppen_code", "igbp_land_use", "plant_functional_type") 
df_sites <- df %>% 
  select(sitename, one_of(meta_vars)) %>% 
  distinct()

vars <- c("TA_F", "TA_F_DAY", "TA_F_NIGHT", "SW_IN_F", "LW_IN_F", "VPD_F", "PA_F", "P_F", "WS_F")

ddf <- df %>% 
  select(-one_of(meta_vars))

write_csv(df_sites, file = "data/df_sites.csv")
write_csv(ddf, file = "data/ddf.csv")
```

```{r}
df_sites <- read_csv("data/df_sites.csv")
ddf <- read_csv("data/ddf.csv")
```

We need only the predictors to pass through the model and then we will calculate the loss by comparing the predictions and the real values. Thus, next step is separating the predictors and the GPP values.

XXX this step is combined below XXX

```{r}
# Separate X and y variables
df_gpp <- df %>% select(GPP_NT_VUT_REF)
df_measure <- df %>% select(-GPP_NT_VUT_REF)
```

Similar to other machine learning task, we split the dataframe into the train and test sets. Note that since we are using a time dependency model, we need to obtain a time-continous train set and test set.

-   XXX You split it by 50%. This is more testing than usual. Is this on purpose? Please explain.

```{r}
idx_train <- seq(ceiling(0.5 * nrow(df)))
ddf_train <- ddf %>% 
  slice(idx_train)
ddf_test <- ddf %>% 
  slice(-idx_train)
```

```{r}
ggplot() +
  geom_line(data = ddf %>% mutate(idx = 1:nrow(.)), aes(idx, GPP_NT_VUT_REF)) +
  geom_vline(xintercept = max(idx_train), linetype = "dashed")
```

<!-- XXX may delete cell below -->

<!-- ```{r} -->

<!-- # determine row indices used for for testing -->

<!-- n <- nrow(df) -->

<!-- test_ind <- c((n-ceiling(n*0.5)):n) -->

<!-- X_train_raw <- df %>%  -->

<!--   slice(-test_ind) %>%  -->

<!--   select(-GPP_NT_VUT_REF) -->

<!-- y_train_raw <- df %>%  -->

<!--   slice(-test_ind) %>%  -->

<!--   select(GPP_NT_VUT_REF) -->

<!-- X_test_raw <-  df %>%  -->

<!--   slice(test_ind) %>%  -->

<!--   select(-GPP_NT_VUT_REF) -->

<!-- y_test_raw <- df %>%  -->

<!--   slice(test_ind) %>%  -->

<!--   select(GPP_NT_VUT_REF) -->

<!-- ``` -->

The meta-data of the site is time invariant which does not pass through our LSTM. We will offer a choice of concatenating it before the MLP layer and we prepare it with one-hot encoding. Normalising the data is required the training. Here we can build some useful functions.

XXX use recipes package (introduce in our course) and perform centering and scaling on training data

```{r}
## define recipe
myrecipe_ddf <- recipe(
  GPP_NT_VUT_REF ~ TA_F + TA_F_DAY + TA_F_NIGHT + SW_IN_F + LW_IN_F + VPD_F + PA_F + P_F + WS_F + wscal + fpar,
  data = ddf_train) %>% 
  step_normalize(all_numeric(), id = "normalize")
  # step_center(all_numeric()) %>%
  # step_scale(all_numeric())

## determine normalisation parameters (mean, sd) on training data (only!)
pp <- prep(myrecipe_ddf, training = ddf_train)

## apply the same normalisation to training and testing
ddf_train_norm <- bake(pp, ddf_train)
ddf_test_norm <- bake(pp, ddf_test)
```

XXX may delete the cells below

```{r }
#Normalising by variables
normalize <- function(df){
  result <- df
  for(name in names(df)[-1]){
    result[name]=(df[name][,1] - mean(df[name][,1])) / sd(df[name][,1])
  }
  return(result)
}

#Separate sensing and meta dataset
prepare_df <- function(data, meta_columns = c("classid","igbp_land_use")){
  df_meta <- data[meta_columns] %>% get_dummies.(meta_columns)
  df_meta <- df_meta[,-c(1,2)]
  sensor_data <- data[!names(data)%in%c("sitename","plant_functional_type","koeppen_code","classid","igbp_land_use")]
  df_sensor <- normalize(sensor_data)
  return(list(df_sensor, df_meta))
}
 
#Computing R2
rsq <- function(x, y){cor(x, y)^2}

```

Standardising and preparing the dataframes. As torch does not like dataframe as input, in the last part we change our "dataframe" into matrix form.

XXX here you determine the mean and sd separately on the training *and* the test set. I think we should apply the parameters determined on the test set on both the test and training set. This is implemented by the recipes steps above.

<!-- ```{r message=F} -->

<!-- # Normalising the GPP values -->

<!-- y_train_normalised <- (y_train_raw - mean(y_train_raw))/sd(y_train_raw) -->

<!-- y_test_normalised <- (y_test_raw - mean(y_test_raw))/sd(y_test_raw) -->

<!-- # Normalising predictors, obtain time dependent and time invariant dataframes -->

<!-- X_train_prep <- prepare_df(X_train_raw) -->

<!-- df_sensor_train <- X_train_prep[[1]] -->

<!-- df_meta_train <- X_train_prep[[2]] -->

<!-- X_test_prep <- prepare_df(X_test_raw) -->

<!-- df_sensor_test <- X_test_prep[[1]] -->

<!-- df_meta_test <- X_test_prep[[2]] -->

<!-- # Glimpes the normalised dataframe -->

<!-- y_train_normalised[0:3] -->

<!-- df_sensor_train[0:3,] -->

<!-- df_meta_train[0:3,] -->

<!-- # Prepare the matrix for training -->

<!-- y_train <- as.matrix(y_train_normalised) -->

<!-- y_test <- as.matrix(y_test_normalised) -->

<!-- x_train <- as.matrix(df_sensor_train) -->

<!-- conditional_train <- as.matrix(df_meta_train) -->

<!-- x_test <- as.matrix(df_sensor_test) -->

<!-- conditional_test <- as.matrix(df_meta_test) -->

<!-- ``` -->

Here is the model LSTM+MLP Architecture. In `initialze` we define the object that is needed for the model. We will use one LSTM followed by three fully connected layers. If we would like to include the meta data, we put `conditional = 1`, then the concatenation is done before the fully connected layers. A linear layer is necessary to map the final layer to the outputs. The function `forward` is defining the architecture of the model that compute from predictors to outputs.

XXX

-   provide more explanations, is `initialize` used to specify the architecture?

-   explain `forward`: how values are passed on between layers?

-   you may drop the `conditional` option. But keep the code for a later chapter where we can use it.

-   Explain the sizes of `in_features` and `out_features`.

```{r LSTM}
mymodule_generator <- nn_module(
  
  initialize = function(input_dim, 
                        # conditional_dim, 
                        hidden_dim, 
                        # conditional, 
                        num_layers = 2
                        ){
    
    self$lstm <- nn_lstm(
      input_size = input_dim, 
      hidden_size = hidden_dim, 
      num_layers = 2, 
      dropout = 0.3
      )
    
    self$conditional <- 0 # conditional
    
    # if (self$conditional == 1){
    #   self$fc1 <- nn_sequential(
    #     nn_linear(
    #       in_features = hidden_dim + conditional_dim, 
    #       out_features = 64
    #       ),
    #     nn_relu()
    #     ) 
    # } else {
    #   self$fc1 <- nn_sequential(
    #     nn_linear(
    #       in_features = hidden_dim, 
    #       out_features = 32), 
    #     nn_relu()
    #     )
    # }

    ## explain numbers and how they relate between layers    
    self$fc1 <- nn_sequential(nn_linear(in_features = hidden_dim, out_features = 32),
                              nn_relu()
                              )
    self$fc2 <- nn_sequential(nn_linear(in_features = 32, out_features = 32), 
                              nn_relu()
                              )    # previously in_features = 64 caused an error
    self$fc3 <- nn_sequential(nn_linear(in_features = 32, out_features = 16), 
                              nn_relu()
                              )
    self$fc4 <- nn_linear(16, 1)
    
  },
  
  forward = function(x){     # , c  XXX removed argument 'c'
    
    ## xxx explain this
    out <- self$lstm(torch_stack(x, 1))
    out <- out[[1]]$squeeze(1)
    
    # if (self$conditional == 1){
    #  out <- torch_cat(list(out,c), dim = 2) 
    # }
    
    y <- self$fc1(out)  
    y <- self$fc2(y)
    y <- self$fc3(y)
    y <- self$fc4(y)
    
  }
)
```

```{r test ,include=FALSE}
#input_dim <- 11
#hidden_dim <- 256

#lstm <- nn_lstm(input_size=input_dim, hidden_size=hidden_dim, num_layers=2, dropout=0.3)
#fc1 <- nn_sequential(nn_linear(in_features=hidden_dim, out_features=64), nn_relu())
#fc2 <- nn_sequential(nn_linear(in_features=64, out_features=32), nn_relu())
#fc3 <- nn_sequential(nn_linear(in_features=32, out_features=16), nn_relu())
#fc4 <- nn_linear(16, 1)
#x <- x_train
#dim(x)
#c=conditional_train
#dim(c)
#out <- lstm(torch_stack(x,1))
#dim(out[[1]])
#out = torch_cat(list(out[[1]]$squeeze(1),c), dim=2) 

#out[[2]]
#y <- fc1(out[[1]])
#x <- fc2(y)
#o <- fc3(x)
#p <- fc4(o)
#dim(p$squeeze(1))

```

We define the parameters for the model. The `INPUT_FEATURES` is number of predictors and we use here 256 hidden nodes (`HIDDEN_DIM`) for the LSTM. `CONDITIONAL_FEATURES` is the dimention of the metadata.

XXX explain 256: how does it relate to number of nodes in our hidden layers

```{r}
# fixed seed
torch_manual_seed(40)

# define number of epoch we want to run
n_epochs <- 30

# the device we want to run the model # TODO use $cuda() to summit to gpus
DEVICE <- if (cuda_is_available()){c("cuda")} else {c("cpu")}

# model parameters
INPUT_FEATURES <- ncol(ddf_train_norm %>% 
                         select(-GPP_NT_VUT_REF)
                       ) # ncol(df_sensor_train)
HIDDEN_DIM <- 256
# CONDITIONAL_FEATURES <- ncol(df_meta_train)
conditional <- 0
```

We put the parameter together with model architecture and obtain the corresponding optimizer.

```{r}
# define model and optimizer
model <- mymodule_generator(
  INPUT_FEATURES,
  # CONDITIONAL_FEATURES, 
  HIDDEN_DIM,
  1 ## is this for num_layers? it seems to be hard-coded in the model specification
  )$to(device = DEVICE)

optimizer <- optim_adam(model$parameters, lr = 0.1)
```

Here is the training step.

```{r}
# training model with defined epochs
r2 <- c()
train_loss <- 0.0
train_r2 <- 0.0

for (epoch in 1:n_epochs){
  
    # put the model in train mode in which parameters are learnt in the backward pass
    model$train()
    
    # prepare dataset and put it into tensor
    # x <- x_train
    # y <- y_train
    # conditional <- conditional_train
    x <- torch_tensor(ddf_train_norm %>% 
                        select(-GPP_NT_VUT_REF) %>% 
                        as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    
    y <- torch_tensor(ddf_train_norm %>% 
                        select(GPP_NT_VUT_REF) %>% 
                        as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    # c <- torch_tensor(conditional, 
    #                   dtype = torch_float()
    #                   )$to(device = DEVICE)
    
    # y_pred <- model(x, c)
    y_pred <- model(x)
    optimizer$zero_grad()
    
    # compute the mse loss
    loss <- nnf_mse_loss(y_pred, y)
    
    # update the parameter
    loss$backward()
    optimizer$step()
    
    # compute the loss and R2 in the training
    train_loss <- train_loss + loss$item()
    train_r2 <- train_r2 + rsq(as.numeric(y), as.numeric(y_pred))

    # put the model in evaluation mode in which the parameters will not be updated.
    model$eval()
    
    # pass the test set into the model and compute the R2
    with_no_grad({
            x <- torch_tensor(ddf_test_norm %>% 
                                select(-GPP_NT_VUT_REF) %>% 
                                as.matrix(), # x_test, 
                              dtype = torch_float()
                              )#$cuda()
            y <- torch_tensor(ddf_test_norm %>% 
                                select(GPP_NT_VUT_REF) %>% 
                                as.matrix(), # y_test, 
                              dtype = torch_float()
                              )#$cuda()
            
#            c <- torch_tensor(conditional_test, 
#                              dtype = torch_float()
#                              )#$cuda()
            
            #y_pred <- model(x, c)
            y_pred <- model(x)
            
            #compute the loss
            test_r2 <- rsq(as.numeric(y), as.numeric(y_pred))
            r2 <- c(r2, test_r2)
            
            #save the predictions 
            if (test_r2 >= max(r2)){cv_pred <- list(y_pred)}
            })
}

```

Printing the result with names of the site and the r2 we got.

```{r message=F}
library(R.utils)
# R2 for the model
printf(c("Site: %s\n", "R2: %s\n"), c(as.character(df$sitename[1]), max(r2)))
```

Visualise results.

```{r}
cv_pred[[1]] %>% as.numeric() %>% hist()

mean <- tidy(pp, id = "normalize") %>%
  filter(terms == "GPP_NT_VUT_REF", statistic == "mean") %>%
  pull(value)

sd <- tidy(pp, id = "normalize") %>%
  filter(terms == "GPP_NT_VUT_REF", statistic == "sd") %>%
  pull(value)

ddf_train_norm %>% 
  mutate(GPP_NT_VUT_REF = GPP_NT_VUT_REF * sd + mean,
         pred = NA
         ) %>% 
  rbind(
    ddf_test_norm %>% 
      mutate(pred = cv_pred[[1]] %>% as.numeric()) %>% 
      mutate(GPP_NT_VUT_REF = GPP_NT_VUT_REF * sd + mean,
             pred = pred * sd + mean)
    ) %>% 
  mutate(idx = 1:nrow(.)) %>% 
  ggplot() +
  geom_line(aes(idx, GPP_NT_VUT_REF)) +
  geom_line(aes(idx, pred), color = "red") +
  geom_vline(xintercept = max(idx_train), linetype = "dashed")

```

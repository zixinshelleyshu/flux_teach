---
editor_options: 
  markdown: 
    wrap: 72
---

# LSTM in Flux Tutorial {#ch-14}

### Motivation

Machine learning and deep learning allows us to model the complex
relationship between predictors and target variables. However, many
popular models are time-agnostic such as random forest (RF) and
fully-connected deep neural networks (DNN). They assume that data has no
temporal dependency and we can shuffle the inputs without influencing
the outputs. This assumption is not true in a lot of the studies. When
we record the measurements in a range of time-span, we sometimes expect
to see a trend depending on the collected time. Often, data naturally
contains the time-dependency. Thus, it will be useful to have a model
considering these temporal dependencies along with time-invariant
metadata. In this tutorial, we will learn about using Long Short-Term
Memory cells in deep neural network architectures to achieve this goal.

We again use the FLUXNET dataset in this chapter. The measurements are
at daily resolution and are used together with (time-invariant) metadata
from each site to predict `GPP_NT_VUT_REF`. We will use a new machine
learning framework called `torch` developed based on `PyTorch`. It is a
neural network library that computes the differentiation automatically
and is powerful in GPU acceleration.

### LSTM

A Long Short-Term Memory (LSTM) neural network is a type of recurrent
neural network. The inputs and outputs of LSTM are time-depended
sequences. Compared to a simple RNN model, the *vanishing gradient
problem* is solved. Here are the details of the LSTM cell. For every
time stamp we pass the data through such a cell to compute the outputs.
A more detailed explanation of LSTMs is given by [this post of Colah's
Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).

![where $h_t$ is the hidden state at time *t*, $c_t$ is the cell state at
time *t*, $x_t$ is the input at time *t*, $h_{(t-1)}$ is the hidden
state of the previous layer at time $t-1$ or the initial hidden state at
time 0, and $i_t, f_t, \tilde{C}_t, o_t$ are the input, forget, cell,
and output gates, respectively. $\sigma$ is the sigmoid function. Here is the detailed expression of the elements in equations.](graph/lstm_withsymbol.png)

$$i_t =  \sigma(W_{ii}x_t + b_{ii} +W_{hi}h(t-1) + b_{hi})$$
$$f_t = \sigma(W_{if}x_t + b_{if} +W_{hf}h(t-1) + b_{hf})$$

$$\tilde{C}_t = tanh(W_{ig}x_t + b_{ig}+W_{hg}h(t-1) + b_{hg})$$
$$o_t = \sigma(W_{io}x_t + b_{io} +W_{ho}h(t-1) + b_{ho})$$
$$C_t = f_tC_{(t-1)} + i_tg_t$$ $$h_t = o_{t}tanh(c_t)$$



### MLP

Multi-layer perception model is a type of artificial neural network
(ANN). Every neural in a single layer is connected to all the neural in
the next layer, so you can also see it is refereed as fully-connected
neural network in some context. The edges are where we call the weights
which are the important parameters to be learned in the training. Every
layer contains the linear mapping and a non-linear "activation
function", except for the output layer. The output layer applies a
linear function to give rise to the output.

![This is the diagram of the connection of LSTM and MLP we used.](graph/mlp_diagram.png)

As LSTM only take care of the time-dependent values, we use a combined
LSTM+MLP to handle both type of data. The concatenated time invariant
variables and the outputs of the hidden layer in LSTM used as the inputs
of MLP. At the training step of such model, in the `forward` pass, we
map inputs and the initial value of the weights to the outputs. In the
`backward` pass we need to do two things to learn the parameter. One is
computing the derivative with respect to the parameter by chain rule and
the other is updating the values with chosen optimization function. As
you can see later, these two steps are corresponding to
`loss$backward()` and `optimizer$step()` in torch. A commonly seen
optimization function is *gradient descent*.

$$\boldsymbol{w}_{n+1}=\boldsymbol{w}_{n}-\gamma \nabla f(\boldsymbol{w}_{n})$$

where $\boldsymbol{w}$ is the parameter, $\gamma$ is the learning rate
and $\boldsymbol{w}_{n}$ is the derivative with respect to the
parameter.

#### Example of apply the model on one site data

We first start with a toy example of using LSTM+MLP with the data from
one site. Let's start with importing the needed packages.

```{r install packages, warning=F, message=F}
library(torch)
library(luz)
library(tidytable)
library(tidyverse)
library(skimr)
library(recipes)
```

Next, we import the dataset which including the daily measurements of
one site and glimpse the summaries of the dataset.

```{r import data, warning=F, message=F}
df <- read_csv("data/df_CH-Lae.csv")
skim(df)
```

Since the meta info of site does not change with time, we would like to
keep temporal data and site metadata in separate files and handle data
frames separately.

```{r}
# define the meta variables and extract the site dataframe
meta_vars <- c("classid", "koeppen_code", "igbp_land_use", "plant_functional_type") 
df_sites <- df %>% 
  select(sitename, one_of(meta_vars)) %>% 
  distinct()

# define the measurement variables and extract the sensor dataframe
vars <- c("TA_F", "TA_F_DAY", "TA_F_NIGHT", "SW_IN_F", "LW_IN_F", "VPD_F", "PA_F", "P_F", "WS_F")
ddf <- df %>% 
  select(-one_of(meta_vars), -"date")

# save the dataframe for further analysis 
write_csv(df_sites, file = "data/df_sites.csv")
write_csv(ddf, file = "data/ddf.csv")
```

```{r, message=F}
# import the data
df_sites <- read_csv("data/df_sites.csv")
ddf <- read_csv("data/ddf.csv")
```

For neural networks, we only pass the predictors through the model, thus
in the next step we separate the target variable (GPP values) from the
dataset. The GPP values will be used in the step of calculating the loss
of the model to update/learn the parameters.

```{r}
# Separate X and y variables
df_target <- df %>% select(GPP_NT_VUT_REF)
df_predictors <- df %>% select(-GPP_NT_VUT_REF)
```

Similar to other machine learning task, we split the dataframe into the
train and test sets. Note that since we are using a time dependency
model, we need to obtain a time-continuous train set and test set.

```{r}
idx_train <- seq(ceiling(0.7 * nrow(df)))
ddf_train <- ddf %>% 
  slice(idx_train)
ddf_test <- ddf %>% 
  slice(-idx_train)
```

```{r}
ggplot() +
  geom_line(data = ddf %>% mutate(idx = 1:nrow(.)), aes(idx, GPP_NT_VUT_REF)) +
  geom_vline(xintercept = max(idx_train), linetype = "dashed")+ggtitle("Visualising the train and test split")
```

We derive normalizing/centering/scaling parameters with the recipes
package based on the training data, and apply them to normalize both
training and testing data.

```{r}
## define recipe
myrecipe_ddf <- recipe(
  GPP_NT_VUT_REF ~ TA_F + TA_F_DAY + TA_F_NIGHT + SW_IN_F + LW_IN_F + VPD_F + PA_F + P_F + WS_F + wscal + fpar,
  data = ddf_train) %>% 
  step_normalize(all_numeric(), id = "normalize")
  # step_center(all_numeric()) %>%
  # step_scale(all_numeric())

## determine normalisation parameters (mean, sd) on training data (only!)
pp <- prep(myrecipe_ddf, training = ddf_train)

## apply the same normalisation to training and testing
ddf_train_norm <- bake(pp, ddf_train)
ddf_test_norm <- bake(pp, ddf_test)

```

Now, we define is the model LSTM+MLP architecture. In `initialze` we
define the objects that is needed for the model, including the function
and the variables. We will use one LSTM followed by three fully
connected layers. A final linear layer is necessary to map the last
hidden layer to the outputs. The function `forward` is defining the
architecture of the model that compute from predictors to outputs and
how we pass the values through every layers. `in_features` and
`out_features` are the number of edges attach to the node at that layer.
Note that since in this case, the metadata is the same for all the
points, how to utilize the metadata in the model will be introduced in
the next section.

```{r LSTM}
mymodule_generator <- nn_module(
  
  initialize = function(input_dim, 
                        hidden_dim, 
                        num_layers = 1 # we have a default of 1 here
                        ){
    
    self$lstm <- nn_lstm(
      input_size = input_dim, 
      hidden_size = hidden_dim, 
      num_layers = num_layers
      )
    
    ## in_feature is the dimension of the last layer and out_feature is the dimension of the next layer
    self$fc1 <- nn_sequential(nn_linear(in_features = hidden_dim, 
                                        out_features = 32),
                              nn_relu()
                              )
    self$fc2 <- nn_sequential(nn_linear(in_features = 32, 
                                        out_features = 32), 
                              nn_relu()
                              )    
    self$fc3 <- nn_sequential(nn_linear(in_features = 32, 
                                        out_features = 16), 
                              nn_relu()
                              )
    self$fc4 <- nn_linear(16, 1)
    
  },
  
  forward = function(x){    
    # concatenate the sequence of tensors by row
    out <- self$lstm(torch_stack(x, 1)) 

    # take the output in hidden state                   
    out <- out[[1]]$squeeze(1) # squeeze here is to squeeze on the unused dimension
    
    y <- self$fc1(out)  
    y <- self$fc2(y)
    y <- self$fc3(y)
    y <- self$fc4(y)
    
  }
)
```

```{r test ,include=FALSE}
#input_dim <- 11
#hidden_dim <- 256

#lstm <- nn_lstm(input_size=input_dim, hidden_size=hidden_dim, num_layers=1)
#fc1 <- nn_sequential(nn_linear(in_features=hidden_dim, out_features=64), nn_relu())
#fc2 <- nn_sequential(nn_linear(in_features=64, out_features=32), nn_relu())
#fc3 <- nn_sequential(nn_linear(in_features=32, out_features=16), nn_relu())
#fc4 <- nn_linear(16, 1)
#x <- x_train
#dim(x)
#c=conditional_train
#dim(c)

#out <- lstm(torch_stack(x,1))
#length(out)

#dim(out[[1]])
#out = torch_cat(list(out[[1]]$squeeze(1),c), dim=2) 

#out[[2]]
#y <- fc1(out[[1]])
#x <- fc2(y)
#o <- fc3(x)
#p <- fc4(o)
#dim(p$squeeze(1))

```

We define the parameters for the model. The `INPUT_FEATURES` is number
of predictors and we use here 256 hidden nodes (`HIDDEN_DIM`) in the
output of the LSTM to MLP.

```{r}
# fixed seed
torch_manual_seed(40)

# define number of epoch we want to run
n_epochs <- 30

# the device we want to run the model
DEVICE <- if (cuda_is_available()){c("cuda")} else {c("cpu")}

# model parameters
INPUT_FEATURES <- ncol(ddf_train_norm %>% 
                         select(-GPP_NT_VUT_REF)
                       ) 
HIDDEN_DIM <- 256 # hyperparameter of connecting LSTM and MLP which is the dimension of the hidden layer that is connecting them

```

We put the parameter together with model architecture and obtain the
corresponding optimizer.

```{r}
# define model and optimizer
model <- mymodule_generator(
  INPUT_FEATURES,
  HIDDEN_DIM,
  num_layers = 1 
  )$to(device = DEVICE)

# we use an optimizer called Adam
optimizer <- optim_adam(model$parameters, lr = 0.1)#

# Prepare the function computing R2
rsq <- function(x, y){cor(x, y)^2}
```

Here is the training step.

```{r}
# training model with defined epochs
r2 <- c()
train_loss <- 0.0
train_r2 <- 0.0

for (epoch in 1:n_epochs){
  
    # put the model in train mode in which parameters are learnt in the backward pass
    model$train()
  
    # Since "torch" operates on the data type "tensor" and the function only accept R atomic vector. So we transform the dataframe to matrix then to tensor data type 
    x <- torch_tensor(ddf_train_norm %>% 
                        select(-GPP_NT_VUT_REF) %>% 
                        as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    
    y <- torch_tensor(ddf_train_norm %>% 
                        select(GPP_NT_VUT_REF) %>% 
                        as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    
    y_pred <- model(x)
    
    # clears old gradients from the last step 
    optimizer$zero_grad()
    
    # specify MSE as the loss function
    loss <- nnf_mse_loss(y_pred, y)
    
    # update the parameter
    loss$backward()
    optimizer$step()
    
    # compute the loss and R2 in the training
    train_loss <- train_loss + loss$item()# extract the loss value as float
    train_r2 <- train_r2 + rsq(as.numeric(y), as.numeric(y_pred))

    # put the model in evaluation mode in which the parameters will not be updated.
    model$eval()
    
    # pass the test set into the model and compute the R2
    # 'with_no_grad()' deactives the gradient calculations which reducing the memory usage and speeding up the computation.
    with_no_grad({
            x <- torch_tensor(ddf_test_norm %>% 
                                select(-GPP_NT_VUT_REF) %>% 
                                as.matrix(), # x_test, 
                              dtype = torch_float()
                              )#$cuda()
            y <- torch_tensor(ddf_test_norm %>% 
                                select(GPP_NT_VUT_REF) %>% 
                                as.matrix(), # y_test, 
                              dtype = torch_float()
                              )#$cuda()
            
            y_pred <- model(x)
            
            # compute the loss
            test_r2 <- rsq(as.numeric(y), as.numeric(y_pred))
            r2 <- c(r2, test_r2)
            
            # save the predictions 
            if (test_r2 >= max(r2)){cv_pred <- list(y_pred)}
            })
}

```

Printing the result with names of the site and the r2 we got.

```{r message=F}
library(R.utils)
# R2 for the model
printf(c("Site: %s\n", "R2: %s\n"), c(as.character(df$sitename[1]), max(r2)))
```

Visualise results.

```{r  ,warning=F,message=F}
cv_pred[[1]] %>% as.numeric() %>% hist()

mean <- tidy(pp, id = "normalize") %>%
  filter(terms == "GPP_NT_VUT_REF", statistic == "mean") %>%
  pull(value)

sd <- tidy(pp, id = "normalize") %>%
  filter(terms == "GPP_NT_VUT_REF", statistic == "sd") %>%
  pull(value)

tmp <- ddf_train_norm %>% 
  mutate(GPP_NT_VUT_REF = GPP_NT_VUT_REF * sd + mean,
         pred = NA
         ) %>% 
  rbind(
    ddf_test_norm %>% 
      mutate(pred = cv_pred[[1]] %>% as.numeric()) %>% 
      mutate(GPP_NT_VUT_REF = GPP_NT_VUT_REF * sd + mean,
             pred = pred * sd + mean)
    ) %>% 
  mutate(idx = 1:nrow(.))

tmp %>% 
  ggplot() +
  geom_line(aes(idx, GPP_NT_VUT_REF)) +
  geom_line(aes(idx, pred), color = "red") +
  geom_vline(xintercept = max(idx_train), linetype = "dashed")

tmp %>% 
  ggplot(aes(pred, GPP_NT_VUT_REF)) +
  geom_hex() +
  geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  scale_fill_gradientn(
    colours = colorRampPalette( c("gray65", "navy", "red", "yellow"))(5)) +
  xlim(-5, 20) + ylim(-5, 20) +
  theme_classic()
    

```


# LSTM with condition {#ch-15}
In this chapter we will introduce a model which includes the time-invariant variables to compute the predictions. We can use this model to predict the GPP values on a new site. Thus, we evaluate it with the predictions of an unseen site. Here is a overview of the LSTM with condition in which the MLP is slightly tuned for better predictions. The "+" in the diagram means concatenation. 
![LSTM_withcondition](graph/lstm_withcond.png)

```{r message=FALSE}
#Import a dataset with multiple sites
df_five<- read_csv("data/df_fivesites.csv")
skim(df_five)
```


```{r}
#outlook the GPP values in the dataset
#df_five<-df_five[df_five$sitename!="US-Var",]
#df_five<-df_five[df_five$sitename!="US-GLE",]
#adding an index within sites with time ordering
#df_withind<-c()
#for(i in 1:5){
#   i_df<-df_test[df_test$sitename==selected_sitename[i],]
#   i_df$inx<-seq(1,nrow(i_df),1)
#   out<-i_df
#   df_withind<-rbind(df_withind,out)}

#ggplot(df_withind, aes(inx, GPP_NT_VUT_REF,colour=sitename))+geom_line()

```

```{r}
selected_sitename<-unique(df_five$sitename)

#adding an index within sites with time ordering
df_withind<-c()
for(i in 1:length(selected_sitename)){
   i_df<-df_five[df_five$sitename==selected_sitename[i],]
   i_df$inx<-seq(1,nrow(i_df),1)
   out<-i_df
   df_withind<-rbind(df_withind,out)}

ggplot(df_withind, aes(inx, GPP_NT_VUT_REF,colour=sitename))+geom_line()

```
Similar to the last chapter we save the time-dependent and time-invariant data separately to prepare for the training. 
```{r}
# define the meta variables and extract the site dataframe
meta_vars <- c("classid", "koeppen_code", "igbp_land_use", "plant_functional_type")

df_sites <- df_five %>% 
  select(sitename, one_of(meta_vars)) 
write_csv(df_sites,"data/df_meta_five.csv")

# define the measurement variables and extract the sensor dataframe
vars <- c("TA_F", "TA_F_DAY", "TA_F_NIGHT", "SW_IN_F", "LW_IN_F", "VPD_F", "PA_F", "P_F", "WS_F")
ddf <-df_five %>% 
  select(-one_of(meta_vars), -"date")
write_csv(ddf,"data/df_sensor_five.csv")
```

```{r}
df_meta_five<-read.csv("data/df_meta_five.csv")
df_sensor_five<-read.csv("data/df_sensor_five.csv")

```
 
 
Since the time-invariant data comes from meta-data of the sites, they are various from site to site. We encode the variables with one-hot encoding method and separate the train and test sets. 
```{r}
#one-hot encoding for the time-invariant variables
df_meta_dummy<-data.frame(get_dummies.(df_meta_five%>%select(-"sitename"))%>%select(-colnames(df_meta_five%>%select(-"sitename"))))
df_meta_dummy$sitename<-df_meta_five$sitename

#select the site that we use for testing
ind_IT_lav<-c(df_five$sitename=="IT-Lav")


#split the train set and test set
meta_five_train<-df_meta_dummy[!ind_IT_lav,]
sensor_five_train<-df_sensor_five[!ind_IT_lav,]


meta_five_test<-df_meta_dummy[ind_IT_lav,]
sensor_five_test<-df_sensor_five[ind_IT_lav,]

```

Similar to the last chapter we apply the recipes package on training and testing datasets. 
```{r}
## define recipe
myrecipe_ddf <- recipe(
  GPP_NT_VUT_REF ~ TA_F + TA_F_DAY + TA_F_NIGHT + SW_IN_F + LW_IN_F + VPD_F + PA_F + P_F + WS_F + wscal + fpar,
  data = sensor_five_train) %>% 
  step_normalize(all_numeric(), id = "normalize")
  # step_center(all_numeric()) %>%
  # step_scale(all_numeric())

## determine normalisation parameters (mean, sd) on training data (only!)
pp <- prep(myrecipe_ddf, training = sensor_five_train)

## apply the same normalisation to training and testing
sensor_five_train_norm <- bake(pp, sensor_five_train)
sensor_five_test_norm <- bake(pp, sensor_five_test)

sensor_five_train_norm$sitename<-sensor_five_train$sitename

```

```{r}
train_sites<-unique(sensor_five_train_norm$sitename)

train_list<-vector(mode = "list", length = length(train_sites))
for(i in 1:length(train_sites)){
  site_df<-sensor_five_train_norm[sensor_five_train_norm$sitename==train_sites[i],]
  train_list[[i]]<-site_df%>%select(-"sitename","GPP_NT_VUT_REF")
}

gpp_train_list<-vector(mode = "list", length = length(train_sites))
for(i in 1:length(train_sites)){
  site_df<-sensor_five_train_norm[sensor_five_train_norm$sitename==train_sites[i],]
  gpp_train_list[[i]]<-site_df%>%select("GPP_NT_VUT_REF")
}

meta_list<-vector(mode = "list", length = length(train_sites))
for(i in 1:length(train_sites)){
  meta_df<-meta_five_train[meta_five_train$sitename==train_sites[i],]
  meta_list[[i]]<-meta_df%>%select(-"sitename")
}

```


Here is the LSTM with condition model. 
```{r LSTM with condition}
mymodule_generator_withcondition <- nn_module(
  initialize = function(input_dim, 
                        conditional_dim, # the dimension of the time-invariant dataset
                        hidden_dim, 
                        conditional=1, # condition=1: we are using condition model, condition=0: we use only sensor data
                        num_layers = 1
                        ){
    
    self$lstm <- nn_lstm(
      input_size = input_dim, 
      hidden_size = hidden_dim, 
      num_layers = num_layers
      )
    
    self$conditional <- conditional
    
    # the input dimension for the MLP is the output of LSTM plus the dimension of time-invariant dataset
    if (self$conditional == 1){
       self$fc1 <- nn_sequential(
        nn_linear(
           in_features = hidden_dim + conditional_dim, 
           out_features =64
           ),
         nn_relu()) 
     } else {
       self$fc1 <- nn_sequential(
         nn_linear(
           in_features = hidden_dim, 
           out_features =64), 
         nn_relu()
         )
     }

     #in_feature is the dimension of the last layer and out_feature is the dimension of the next layer
    
    self$fc2 <- nn_sequential(nn_linear(in_features=64, 
                                        out_features =32), 
                              nn_relu()
                              )
    self$fc3 <- nn_sequential(nn_linear(in_features =32, 
                                        out_features =16), 
                              nn_relu()
                              )
    self$fc4 <- nn_linear(16, 1)
    
  },
  
  forward = function(x,c){    
    
    out <- self$lstm(torch_stack(x, 1)) 

    # take the output in hidden state
    out <- out[[1]]$squeeze(1) # squeeze here is to squeeze on the unused dimension
    
    # we concatenating the time-invariant to the output of LSTM when utilizing condition part of the model. 
    if (self$conditional == 1){
      out <- torch_cat(list(out,c), dim = 2) 
     }
    
    y <- self$fc1(out)  
    y <- self$fc2(y)
    y <- self$fc3(y)
    y <- self$fc4(y)
    
  }
)
```


```{r}
# fixed seed
torch_manual_seed(40)

# define number of epoch we want to run
n_epochs <- 30

# the device we want to run the model
DEVICE <- if (cuda_is_available()){c("cuda")} else {c("cpu")}

# model parameters
INPUT_FEATURES <- ncol(sensor_five_train%>% 
                         select(-c("GPP_NT_VUT_REF","sitename")))

HIDDEN_DIM <- 128# hyperparameter of connecting LSTM and MLP which is the dimension of the hidden layer that is connecting them

CONDITIONAL_FEATURES <- ncol(df_meta_dummy)-1# the dimension of the condition feature is the number of column of meta-data after one-hot encoding
conditional <- 1 #the indicator that we will use condition part in the model
```


We put the parameter together with model architecture and obtain the corresponding optimizer.
```{r}
# define model and optimizer
model_condition<- mymodule_generator_withcondition(
  INPUT_FEATURES,
  CONDITIONAL_FEATURES,
  HIDDEN_DIM,
  conditional =1,
  num_layers = 1
  )$to(device = DEVICE)

# we use an optimizer called Adam
optimizer <- optim_adam(model$parameters, lr = 0.01)

# Prepare the function computing R2
rsq <- function(x, y){cor(x, y)^2}
```

```{r}
zip <- function(...) {
  mapply(list, ..., SIMPLIFY = FALSE)
}
```

```{r}
training<-zip(train_list,gpp_train_list, meta_list)
training_list<-training[[1]]

training_list[1]

```

Now we train the LSTM with condition model to predict the left-out site "IT-Lav". 
```{r}
# training model with defined epochs
r2 <- c()
train_loss <- 0.0
train_r2 <- 0.0

for (epoch in 1:n_epochs){
  
  # put the model in train mode in which parameters are learnt in the backward pass
  model_condition$train()
  
  
  for (training_list in zip(train_list,gpp_train_list, meta_list)){
    
    x<-training_list[1][[1]]
    y<-training_list[2][[1]]
    c<-training_list[3][[1]]
    
    # Since "torch" operates on the data type "tensor" and the function only accept R atomic vector. So we transform the dataframe to matrix then to tensor data type 
    x <- torch_tensor(x %>% select(-GPP_NT_VUT_REF) %>%as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    
    
    y <- torch_tensor(y %>% 
                        as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    
    c <- torch_tensor(c %>%
                        as.matrix(), 
                       dtype = torch_float()
                       )$to(device = DEVICE)
    
    
    y_pred <- model_condition(x,c)
    
    # clears old gradients from the last step 
    optimizer$zero_grad()
    
    # specify MSE as the loss function
    loss <- nnf_mse_loss(y_pred, y)
    
    # update the parameter
    loss$backward()
    optimizer$step()
    
    # compute the loss and R2 in the training
    train_loss <- train_loss + loss$item()# extract the loss value as float
    train_r2 <- train_r2 + rsq(as.numeric(y), as.numeric(y_pred)) }
  
  # put the model in evaluation mode in which the parameters will not be updated.
  model_condition$eval()
    
  # pass the test set into the model and compute the R2
  # 'with_no_grad()' deactives the gradient calculations which reducing the memory usage and speeding up the computation.
  with_no_grad({
    x <- torch_tensor(sensor_five_test_norm%>% 
                                select(-GPP_NT_VUT_REF) %>% 
                                as.matrix(), # x_test, 
                              dtype = torch_float()
                              )#$cuda()
    
    y <- torch_tensor(sensor_five_test_norm%>% 
                                select(GPP_NT_VUT_REF) %>% 
                                as.matrix(), # y_test, 
                              dtype = torch_float()
                              )#$cuda()
            
    c <- torch_tensor(meta_five_test%>% select(-sitename)%>%
                                as.matrix(), 
                              dtype = torch_float()
                              )#$cuda()
    
            
    y_pred <- model_condition(x,c)
            
    # compute the loss
    test_r2 <- rsq(as.numeric(y), as.numeric(y_pred))
    r2 <- c(r2, test_r2)
    # save the predictions 
    if (test_r2 >= max(r2)){cv_pred <- list(y_pred)} })
}

```

Print the predicted result
```{r}
printf(c("Site: %s\n", "R2: %s\n"), c(as.character("IT-Lav"), max(r2)))
```

Visualise the results
```{r}
cv_pred[[1]]%>% as.numeric() %>% hist()
```

```{r, message=FALSE}

aaa<-data.frame(GPP_NT_VUT_REF=sensor_five_test_norm$GPP_NT_VUT_REF,pred=cv_pred[[1]] %>% as.numeric())
aaa$ind<-seq(1,nrow(aaa))
rsq(sensor_five_test_norm$GPP_NT_VUT_REF %>% as.numeric(),cv_pred[[1]] %>% as.numeric())
ggplot(aaa)+geom_line(aes(ind,GPP_NT_VUT_REF))+geom_line(aes(ind, pred),color="red")
ggplot(aaa)+geom_line(aes(ind,GPP_NT_VUT_REF))
ggplot(aaa)+geom_line(aes(ind, pred),color="red")
```



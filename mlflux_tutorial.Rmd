---
editor_options:
  markdown:
    wrap: 72
---

# LSTM in Flux Tutorial {#ch-14}

### Motivation

Machine learning and deep learning allows us to model the complex
relationship between predictors and target variables. However, many
popular models are time-agnostic such as random forest (RF) and
fully-connected deep neural networks (DNN). They assume that data has no
temporal dependency and we can shuffle the inputs without influencing
the outputs. This assumption is not true in a lot of the studies. When
we record the measurements in a range of time-span, we sometimes expect
to see a trend depending on the collected time. Often, data naturally
contains the time-dependency. Thus, it will be useful to have a model
considering these temporal dependencies along with time-invariant
metadata. In this tutorial, we will learn about using Long Short-Term
Memory cells in deep neural network architectures to achieve this goal.

We again use the FLUXNET dataset in this chapter. The measurements are
at daily resolution and are used together with (time-invariant) metadata
from each site to predict `GPP_NT_VUT_REF`. We will use a new machine
learning framework called `torch` in R developed based on `PyTorch`. It is a
neural network library that computes the differentiation automatically
and is powerful in GPU acceleration.

### Long Short-Term Memory

A Long Short-Term Memory (LSTM) neural network is a type of recurrent
neural network. The inputs and outputs of LSTM are time-depended
sequences. Compared to a simple RNN model, the *vanishing gradient
problem* is solved. Here are the details of the LSTM cell. For every
time stamp we pass the data through such a cell to compute the outputs.
A more detailed explanation of LSTMs is given by [this post of Colah's
Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).

![where $h_t$ is the hidden state at time *t*, $c_t$ is the cell state at
time *t*, $x_t$ is the input at time *t*, $h_{(t-1)}$ is the hidden
state of the previous layer at time $t-1$ or the initial hidden state at
time 0, and $i_t, f_t, \tilde{C}_t, o_t$ are the input, forget, cell,
and output gates, respectively. $\sigma$ is the sigmoid function. Here is the detailed expression of the elements in equations.](graph/lstm_withsymbol.png)

$$i_t =  \sigma(W_{ii}x_t + b_{ii} +W_{hi}h(t-1) + b_{hi})$$
$$f_t = \sigma(W_{if}x_t + b_{if} +W_{hf}h(t-1) + b_{hf})$$

$$\tilde{C}_t = tanh(W_{ig}x_t + b_{ig}+W_{hg}h(t-1) + b_{hg})$$
$$o_t = \sigma(W_{io}x_t + b_{io} +W_{ho}h(t-1) + b_{ho})$$
$$C_t = f_tC_{(t-1)} + i_tg_t$$ $$h_t = o_{t}tanh(c_t)$$



### Multi-layer perception

Multi-layer perception(MLP) model is a type of artificial neural network
(ANN). Every neural in a single layer is connected to all the neural in
the next layer, so you can also see it is refereed as fully-connected
neural network in some context. The edges are where we call the weights
which are the important parameters to be learned in the training. Every
layer contains the linear mapping and a non-linear "activation
function", except for the output layer. The output layer applies a
linear function to give rise to the output.

![This is the diagram of the connection of LSTM and MLP we used.](graph/mlp_diagram.png)

As LSTM only take care of the time-dependent values, we use a combined
LSTM+MLP to handle both type of data. The concatenated time invariant
variables and the outputs of the hidden layer in LSTM used as the inputs
of MLP. At the training step of such model, in the `forward` pass, we
map inputs and the initial value of the weights to the outputs. In the
`backward` pass we need to do two things to learn the parameter. One is
computing the derivative with respect to the parameter by chain rule and
the other is updating the values with chosen optimization function. As
you can see later, these two steps are corresponding to
`loss$backward()` and `optimizer$step()` in torch. A commonly seen
optimization function is *gradient descent*.

$$\boldsymbol{w}_{n+1}=\boldsymbol{w}_{n}-\gamma \nabla f(\boldsymbol{w}_{n})$$

where $\boldsymbol{w}$ is the parameter, $\gamma$ is the learning rate
and $\boldsymbol{w}_{n}$ is the derivative with respect to the
parameter.

#### Example of apply the model on one site data

We first start with a toy example of using LSTM+MLP with the data from
one site. Let's start with importing the needed packages.

```{r install packages, warning=F, message=F}
library(torch)
library(luz)
library(tidytable)
library(tidyverse)
library(skimr)
library(recipes)


```

Next, we import the dataset which including the daily measurements of
one site and glimpse the summaries of the dataset.

```{r import data, warning=F, message=F}
df <- read_csv("data/df_CH-Lae.csv")
skim(df)
```

Since the meta info of site does not change with time, we would like to
keep temporal data and site metadata in separate files and handle data
frames separately.

```{r}
# define the meta variables and extract the site dataframe
meta_vars <- c("classid", "koeppen_code", "igbp_land_use", "plant_functional_type") 
df_sites <- df %>% 
  select(sitename, one_of(meta_vars)) %>% 
  distinct()

# define the measurement variables and extract the sensor dataframe
vars <- c("TA_F", "TA_F_DAY", "TA_F_NIGHT", "SW_IN_F", "LW_IN_F", "VPD_F", "PA_F", "P_F", "WS_F")
ddf <- df %>% 
  select(-one_of(meta_vars), -"date")

# save the dataframe for further analysis 
write_csv(df_sites, file = "data/df_sites.csv")
write_csv(ddf, file = "data/ddf.csv")
```

```{r, message=F}
# import the data
df_sites <- read_csv("data/df_sites.csv")
ddf <- read_csv("data/ddf.csv")
```

For neural networks, we only pass the predictors through the model, thus
in the next step we separate the target variable (GPP values) from the
dataset. The GPP values will be used in the step of calculating the loss
of the model to update/learn the parameters.

```{r}
# Separate X and y variables
df_target <- df %>% select(GPP_NT_VUT_REF)
df_predictors <- df %>% select(-GPP_NT_VUT_REF)
```

Similar to other machine learning task, we split the dataframe into the
train and test sets. Note that since we are using a time dependency
model, we need to obtain a time-continuous train set and test set.

```{r}
idx_train <- seq(ceiling(0.7 * nrow(df)))
ddf_train <- ddf %>% 
  slice(idx_train)
ddf_test <- ddf %>% 
  slice(-idx_train)
```

```{r}
ggplot() +
  geom_line(data = ddf %>% mutate(idx = 1:nrow(.)), aes(idx, GPP_NT_VUT_REF)) +
  geom_vline(xintercept = max(idx_train), linetype = "dashed")+ggtitle("Visualising the train and test split")
```

We derive normalizing/centering/scaling parameters with the recipes
package based on the training data, and apply them to normalize both
training and testing data.

```{r}
## define recipe
myrecipe_ddf <- recipe(
  GPP_NT_VUT_REF ~ TA_F + TA_F_DAY + TA_F_NIGHT + SW_IN_F + LW_IN_F + VPD_F + PA_F + P_F + WS_F + wscal + fpar,
  data = ddf_train) %>% 
  step_normalize(all_numeric(), id = "normalize")
  # step_center(all_numeric()) %>%
  # step_scale(all_numeric())

## determine normalisation parameters (mean, sd) on training data (only!)
pp <- prep(myrecipe_ddf, training = ddf_train)

## apply the same normalisation to training and testing
ddf_train_norm <- bake(pp, ddf_train)
ddf_test_norm <- bake(pp, ddf_test)

```

Now, we define the model LSTM+MLP architecture. In `initialze` we
define the objects that is needed for the model, including the function
and the variables. We will use one LSTM followed by three fully
connected layers. A final linear layer is necessary to map the last
hidden layer to the outputs. The function `forward` is defining the
architecture of the model that compute from predictors to outputs and
how we pass the values through every layers. `in_features` and
`out_features` are the number of edges attach to the node at that layer.
Note that since in this case, the metadata is the same for all the
points, how to utilize the metadata in the model will be introduced in
the next chapter.

```{r LSTM}
mymodule_generator <- nn_module(
  
  initialize = function(input_dim, 
                        hidden_dim, 
                        num_layers = 1 # we have a default of 1 here
                        ){
    
    self$lstm <- nn_lstm(
      input_size = input_dim, 
      hidden_size = hidden_dim, 
      num_layers = num_layers
      )
    
    ## in_feature is the dimension of the last layer and out_feature is the dimension of the next layer
    ## relu is the name of used activation function
    self$fc1 <- nn_sequential(nn_linear(in_features = hidden_dim, 
                                        out_features = 32),
                              nn_relu()
                              )
    self$fc2 <- nn_sequential(nn_linear(in_features = 32, 
                                        out_features = 32), 
                              nn_relu()
                              )    
    self$fc3 <- nn_sequential(nn_linear(in_features = 32, 
                                        out_features = 16), 
                              nn_relu()
                              )
    self$fc4 <- nn_linear(16, 1)
    
  },
  
  forward = function(x){    
    # concatenate the sequence of tensors by row
    out <- self$lstm(torch_stack(x, 1)) 

    # take the values in the output of LSTM layer         
    out <- out[[1]]$squeeze(1) # squeeze here is to squeeze on the unused dimension
    
    y <- self$fc1(out)  
    y <- self$fc2(y)
    y <- self$fc3(y)
    y <- self$fc4(y)
    
  }
)
```

```{r test ,include=FALSE}
#input_dim <- 11
#hidden_dim <- 256

#lstm <- nn_lstm(input_size=input_dim, hidden_size=hidden_dim, num_layers=1)
#fc1 <- nn_sequential(nn_linear(in_features=hidden_dim, out_features=64), nn_relu())
#fc2 <- nn_sequential(nn_linear(in_features=64, out_features=32), nn_relu())
#fc3 <- nn_sequential(nn_linear(in_features=32, out_features=16), nn_relu())
#fc4 <- nn_linear(16, 1)
#x <- x_train
#dim(x)
#c=conditional_train
#dim(c)

#out <- lstm(torch_stack(x,1))
#length(out)

#dim(out[[1]])
#out = torch_cat(list(out[[1]]$squeeze(1),c), dim=2) 

#out[[2]]
#y <- fc1(out[[1]])
#x <- fc2(y)
#o <- fc3(x)
#p <- fc4(o)
#dim(p$squeeze(1))

```

We define the parameters for the model and training. The `INPUT_FEATURES` is number
of predictors and we use here 256 hidden nodes (`HIDDEN_DIM`) in the
output of the LSTM to MLP.

```{r}
# fixed seed
torch_manual_seed(40)

# define number of epoch we want to run
n_epochs <- 30

# the device we want to run the model
DEVICE <- if (cuda_is_available()){c("cuda")} else {c("cpu")}

# model parameters
INPUT_FEATURES <- ncol(ddf_train_norm %>% 
                         select(-GPP_NT_VUT_REF)
                       ) 
HIDDEN_DIM <- 256 # hyperparameter of connecting LSTM and MLP which is the dimension of the hidden layer that is connecting them

```

We put the parameter together with model architecture and obtain the
corresponding optimizer.

```{r}
# define model and optimizer
model <- mymodule_generator(
  INPUT_FEATURES,
  HIDDEN_DIM,
  num_layers = 1 
  )$to(device = DEVICE)

# we use an optimizer called Adam
optimizer <- optim_adam(model$parameters, lr = 0.1)#

# Prepare the function computing R2
rsq <- function(x, y){cor(x, y)^2}
```

Here is the training step.

```{r}
# training model with defined epochs
r2 <- c()
train_loss <- 0.0
train_r2 <- 0.0

for (epoch in 1:n_epochs){
  
    # put the model in train mode in which parameters are learnt and updated in the backward pass
    model$train()
  
    # Since "torch" operates on the data type "tensor" and the function only accept R atomic vector. So we transform the dataframe to matrix then to tensor data type 
    x <- torch_tensor(ddf_train_norm %>% 
                        select(-GPP_NT_VUT_REF) %>% 
                        as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    
    y <- torch_tensor(ddf_train_norm %>% 
                        select(GPP_NT_VUT_REF) %>% 
                        as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    
    y_pred <- model(x)
    
    # clears old gradients from the last step 
    optimizer$zero_grad()
    
    # specify MSE as the loss function and calculate it in every epoch
    loss <- nnf_mse_loss(y_pred, y)
    
    # update the weights
    loss$backward() #calculating gradient
    optimizer$step() #update with optimizer 
    
    # compute the loss and R2 in the training
    train_loss <- train_loss + loss$item()#extract the loss value as float and calculate the sum of training losses (can be divided by loss to calculate the average training loss)
    train_r2 <- c(train_r2, rsq(as.numeric(y), as.numeric(y_pred)))#check the R2 per epoch

    # put the model in evaluation mode in which the parameters will not be updated.
    model$eval()
    
    # pass the test set into the model and compute the R2
    # 'with_no_grad()' de-actives the gradient calculations which reducing the memory usage and speeding up the computation.
    with_no_grad({
            x <- torch_tensor(ddf_test_norm %>% 
                                select(-GPP_NT_VUT_REF) %>% 
                                as.matrix(), # x_test, 
                              dtype = torch_float()
                              )#$cuda()
            y <- torch_tensor(ddf_test_norm %>% 
                                select(GPP_NT_VUT_REF) %>% 
                                as.matrix(), # y_test, 
                              dtype = torch_float()
                              )#$cuda()
            
            y_pred <- model(x)
            
            # compute the loss
            test_r2 <- rsq(as.numeric(y), as.numeric(y_pred))
            r2 <- c(r2, test_r2)
            
            # save the predictions 
            if (test_r2 >= max(r2)){cv_pred <- list(y_pred)}
            })
}

```

Printing the result with names of the site and the r2 we got.

```{r message=F}
library(R.utils)
# R2 for the model
printf(c("Site: %s\n", "R2: %s\n"), c(as.character(df$sitename[1]), max(r2)))
```

Visualise results.

```{r  ,warning=F,message=F}
cv_pred[[1]] %>% as.numeric() %>% hist(main="Prediction of LSTM+MLP on GPP values")

mean <- tidy(pp, id = "normalize") %>%
  filter(terms == "GPP_NT_VUT_REF", statistic == "mean") %>%
  pull(value)

sd <- tidy(pp, id = "normalize") %>%
  filter(terms == "GPP_NT_VUT_REF", statistic == "sd") %>%
  pull(value)

tmp <- ddf_train_norm %>% 
  mutate(GPP_NT_VUT_REF = GPP_NT_VUT_REF * sd + mean,
         pred = NA
         ) %>% 
  rbind(
    ddf_test_norm %>% 
      mutate(pred = cv_pred[[1]] %>% as.numeric()) %>% 
      mutate(GPP_NT_VUT_REF = GPP_NT_VUT_REF * sd + mean,
             pred = pred * sd + mean)
    ) %>% 
  mutate(idx = 1:nrow(.))

tmp %>% 
  ggplot() +
  geom_line(aes(idx, GPP_NT_VUT_REF)) +
  geom_line(aes(idx, pred), color = "red") +
  geom_vline(xintercept = max(idx_train), linetype = "dashed")

tmp %>% 
  ggplot(aes(pred, GPP_NT_VUT_REF)) +
  geom_hex() +
  geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  scale_fill_gradientn(
    colours = colorRampPalette( c("gray65", "navy", "red", "yellow"))(5)) +
  xlim(-5, 20) + ylim(-5, 20) +
  theme_classic()
    

```


# LSTM with condition {#ch-15}
In this chapter we will introduce a model gives an option of incorporating the time-invariant variables to compute the predictions. The time-invariant variables are usually information of sites. We would like to examine the generalisability of out-of-distribution data which means it has the capability to predict on the GPP values on an unseen site. Thus, we train the model on data from multiple sites and evaluate on an new site. Here is an overview of the "LSTM with condition" in which the MLP is slightly tuned for better predictions. The "+" in the diagram means concatenation. We encode the vegetation types with "one-hot-encoding" method. 
![LSTM_withcondition](graph/lstm_withcond.png)
First we import the data with multiple sites. 
```{r message=FALSE}
#Import a dataset with multiple sites
df_multi<- read_csv("data/df_multi.csv")
skim(df_multi)
```


```{r include=FALSE}
#outlook the GPP values in the dataset
#df_five<-df_five[df_five$sitename!="US-Var",]
#df_five<-df_five[df_five$sitename!="US-GLE",]
#adding an index within sites with time ordering
#df_withind<-c()
#for(i in 1:5){
#   i_df<-df_test[df_test$sitename==selected_sitename[i],]
#   i_df$inx<-seq(1,nrow(i_df),1)
#   out<-i_df
#   df_withind<-rbind(df_withind,out)}

#ggplot(df_withind, aes(inx, GPP_NT_VUT_REF,colour=sitename))+geom_line()

```

Then we have a feeling of the target values by visualising with ggplot. We can see a seasonal trend in the data and the time-span of the data is slightly different from site to site. 
```{r}
#get the name of sites
selected_sitename<-unique(df_multi$sitename)

#adding an index within sites with time ordering
df_withind<-c()
for(i in 1:length(selected_sitename)){
   i_df<-df_multi[df_multi$sitename==selected_sitename[i],]
   i_df$inx<-seq(1,nrow(i_df),1)
   out<-i_df
   df_withind<-rbind(df_withind,out)}

ggplot(df_withind, aes(inx, GPP_NT_VUT_REF,colour=sitename))+geom_line()+ggtitle("visualising target value by sites")

```

Similar to the last chapter we save the time-dependent and time-invariant data separately to prepare for the training. 
```{r}
# define the meta variables and extract the site dataframe
meta_vars <- c("classid", "koeppen_code", "igbp_land_use", "plant_functional_type")

df_sites <- df_multi %>% 
  select(sitename, one_of(meta_vars)) 
write_csv(df_sites,"data/df_meta_multi.csv")

# define the measurement variables and extract the sensor dataframe
vars <- c("TA_F", "TA_F_DAY", "TA_F_NIGHT", "SW_IN_F", "LW_IN_F", "VPD_F", "PA_F", "P_F", "WS_F")
ddf <-df_multi %>% 
  select(-one_of(meta_vars), -"date")
write_csv(ddf,"data/df_sensor_multi.csv")
```

```{r}
df_meta_multi<-read.csv("data/df_meta_multi.csv")
df_sensor_multi<-read.csv("data/df_sensor_multi.csv")
```
 
The time-invariant variables are different in every site. We encode them with one-hot encoding and separate the train and test sets. 
```{r}
#one-hot encoding for the time-invariant variables. we only use classid and kopeppen_code here.
df_meta_dummy<-data.frame(get_dummies.(df_meta_multi%>%select(-c("sitename","koeppen_code","plant_functional_type")))%>%select(-colnames(df_meta_multi%>%select(-c("sitename","koeppen_code","plant_functional_type")))))
                                                                                                                             
df_meta_dummy$sitename<-df_meta_multi$sitename

#select the site that we use for testing and get its index
ind_CH_Lae<-c(df_multi$sitename=="CH-Lae")

#split the train set and test set. This time we take the whole site out as the test site. 
meta_multi_train<-df_meta_dummy[!ind_CH_Lae,]
sensor_multi_train<-df_sensor_multi[!ind_CH_Lae,]

meta_multi_test<-df_meta_dummy[ind_CH_Lae,]
sensor_multi_test<-df_sensor_multi[ind_CH_Lae,]

```

Similarly, we apply the recipes package on train and test sensing data for normalisation.  
```{r}
## define recipe
myrecipe_ddf <- recipe(
  GPP_NT_VUT_REF ~ TA_F + TA_F_DAY + TA_F_NIGHT + SW_IN_F + LW_IN_F + VPD_F + PA_F + P_F + WS_F + wscal + fpar,
  data = sensor_multi_train) %>% 
  step_normalize(all_numeric(), id = "normalize")
  # step_center(all_numeric()) %>%
  # step_scale(all_numeric())

## determine normalisation parameters (mean, sd) on training data (only!)
pp <- prep(myrecipe_ddf, training = sensor_multi_train)

## apply the same normalisation to training and testing
sensor_multi_train_norm <- bake(pp, sensor_multi_train)
sensor_multi_test_norm <- bake(pp, sensor_multi_test)
```


We want to keep the time-dependence by every site but not crossing sites when passing through LSTM, so we wrap the data by sites here.
```{r}
#we add back the sitename column
sensor_multi_train_norm$sitename<-sensor_multi_train$sitename

#get the sitename in training set
train_sites<-unique(sensor_multi_train_norm$sitename)

# wrap the sensor dataset by site
train_list<-vector(mode = "list", length = length(train_sites))
for(i in 1:length(train_sites)){
  site_df<-sensor_multi_train_norm[sensor_multi_train_norm$sitename==train_sites[i],]
  train_list[[i]]<-site_df%>%select(-"sitename","GPP_NT_VUT_REF")
}

# wrap the gpp value by site
gpp_train_list<-vector(mode = "list", length = length(train_sites))
for(i in 1:length(train_sites)){
  site_df<-sensor_multi_train_norm[sensor_multi_train_norm$sitename==train_sites[i],]
  gpp_train_list[[i]]<-site_df%>%select("GPP_NT_VUT_REF")
}

#wrap the meta dataset by site
meta_list<-vector(mode = "list", length = length(train_sites))
for(i in 1:length(train_sites)){
  meta_df<-meta_multi_train[meta_multi_train$sitename==train_sites[i],]
  meta_list[[i]]<-meta_df%>%select(-"sitename")
}

# binding them to one big set
training_df<-rbind(train_list,gpp_train_list, meta_list)
```

Here is the LSTM with optional condition model. 
```{r LSTM with condition}
mymodule_generator_withcondition <- nn_module(
  initialize = function(input_dim, 
                        conditional_dim, # the dimension of the time-invariant dataset
                        hidden_dim, 
                        conditional=0, # condition=1: we are using condition model, condition=0: we use only sensor data
                        num_layers = 2 #number of layer increases to 2 here to improve performance of the example
                        ){
    
    self$lstm <- nn_lstm(
      input_size = input_dim, 
      hidden_size = hidden_dim, 
      num_layers = num_layers
      )
    
    self$conditional <- conditional
    
    # the input dimension for the MLP is the output of LSTM plus the dimension of time-invariant dataset
    if (self$conditional == 1){
       self$fc1 <- nn_sequential(
        nn_linear(
           in_features = hidden_dim + conditional_dim, # the metadata are concatenated to the output of lstm here
           out_features =64
           ),
         nn_relu()) 
     } else {# optionally when we do not indicate the use of condition the model is similar to chapter 1
       self$fc1 <- nn_sequential(
         nn_linear(
           in_features = hidden_dim, 
           out_features =64), 
         nn_relu()
         )
     }
    
    self$fc2 <- nn_sequential(nn_linear(in_features=64, 
                                       out_features =64), 
                              nn_relu()
                              )
    
    self$fc3 <- nn_sequential(nn_linear(in_features =64, 
                                        out_features =16), 
                              nn_relu()
                              )
    self$fc4 <- nn_linear(16, 1)
    
  },
  
  forward = function(x,c){    
    
    out <- self$lstm(torch_stack(x, 1)) 

    # take the output in hidden state
    out <- out[[1]]$squeeze(1) # squeeze here is to squeeze on the unused dimension
    
    # we concatenating the time-invariant variables to the outputs of LSTM when utilizing condition part of the model. 
    if (self$conditional == 1){
      out <- torch_cat(list(out,c), dim = 2) 
     }
    
    # then this part is the same with last chapter
    y <- self$fc1(out)  
    y <- self$fc2(y)
    y <- self$fc3(y)
    y <- self$fc4(y)
    
  }
)
```

We define the parameter for model and training
```{r}
# fixed seed
torch_manual_seed(40)

# define number of epoch we want to run
n_epochs <- 30

# the device we want to run the model
DEVICE <- if (cuda_is_available()){c("cuda")} else {c("cpu")}

# model parameters
INPUT_FEATURES <- ncol(sensor_multi_train%>% 
                         select(-c("GPP_NT_VUT_REF","sitename")))

HIDDEN_DIM <- 256

CONDITIONAL_FEATURES <- ncol(meta_multi_train)-1 # the dimension of the condition feature is the number of column of meta-data after one-hot encoding and we dont put sitenames in. 

conditional <- 1 #we will use condition part in the model. It is worth to experiment if incorporating the time-invariant model help improve the performance of the model, simply by changing it to 0 or other numbers. 
```

We put the parameter together with model architecture and obtain the corresponding optimizer.
```{r}
# define model and optimizer
model_condition<- mymodule_generator_withcondition(
  INPUT_FEATURES,
  CONDITIONAL_FEATURES,
  HIDDEN_DIM,
  conditional =conditional,
  num_layers = 1
  )$to(device = DEVICE)

# we use an optimizer called Adam
optimizer <- optim_adam(model_condition$parameters, lr = 0.001)

# Prepare the function computing R2
rsq <- function(x, y){cor(x, y)^2}
```


Now we train the LSTM with condition model to predict the left-out site "IT-Lav". 
```{r message=FALSE}
# training model with defined epochs
r2 <- c()

for (epoch in 1:n_epochs){
  train_loss <- 0.0
  train_r2 <- 0.0
  
  model_condition$train()
  
  # we update the parameter of the model after every time passing one site of data
  for (i in 1:ncol(training_df)){
    
    #take out the data from one site
    training_list<-training_df[,i]
    
    x<-training_list$train_list
    y<-training_list$gpp_train_list
    c<-training_list$meta_list
    
    x <- torch_tensor(x %>% select(-GPP_NT_VUT_REF) 
                       %>%as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    
    
    y <- torch_tensor(y %>% 
                        as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    
    c <- torch_tensor(c %>%
                        as.matrix(), 
                       dtype = torch_float()
                       )$to(device = DEVICE)
    
    
    y_pred <- model_condition(x,c)
    
    optimizer$zero_grad()
    
    loss <- nnf_mse_loss(y_pred, y)
    
    # update the parameter after one site of data passing
    loss$backward()
    optimizer$step()
    
    # compute the loss and R2 in the training
    train_loss <- c(train_loss, loss$item())
    train_r2 <- c(train_r2,rsq(as.numeric(y), as.numeric(y_pred)))}
  
  # we evaluate the model after training the model on all sites of data
  model_condition$eval()
    
  # pass the test set into the model and compute the R2
  with_no_grad({
    x <- torch_tensor(sensor_multi_test_norm%>% 
                                select(-GPP_NT_VUT_REF) %>% 
                                as.matrix(), # x_test, 
                              dtype = torch_float()
                              )#$cuda()
    
    y <- torch_tensor(sensor_multi_test_norm%>% 
                                select(GPP_NT_VUT_REF) %>% 
                                as.matrix(), # y_test, 
                              dtype = torch_float()
                              )#$cuda()
            
    c <- torch_tensor(meta_multi_test%>% select(-sitename)%>%
                                as.matrix(), #conditions
                              dtype = torch_float()
                              )#$cuda()
    
            
    y_pred <- model_condition(x,c)
            
    # compute the r2 in the test time
    test_r2 <- rsq(as.numeric(y), as.numeric(y_pred))
    r2 <- c(r2, test_r2)
    
    # save the predictions of the highest r2 epoch
    if (test_r2 >= max(r2)){cv_pred <- list(y_pred)} })
}

```

Print the predicted result
```{r}
printf(c("Site: %s\n", "R2: %s\n"), c(as.character("CH-Lae"), max(r2)))
```

Visualizing the result
```{r message=FALSE, warning=F}
cv_pred[[1]]%>% as.numeric() %>% hist(main="Prediction of LSTM with condition on GPP values of CH-Lae")
```


```{r}
mean <- tidy(pp, id = "normalize") %>%
  filter(terms == "GPP_NT_VUT_REF", statistic == "mean") %>%
  pull(value)

sd <- tidy(pp, id = "normalize") %>%
  filter(terms == "GPP_NT_VUT_REF", statistic == "sd") %>%
  pull(value)

tmp<-sensor_multi_test_norm %>% mutate(pred = cv_pred[[1]] %>% as.numeric()) %>% 
      mutate(GPP_NT_VUT_REF = GPP_NT_VUT_REF * sd + mean,
             pred = pred * sd + mean) %>% mutate(ind =seq(1,nrow(sensor_multi_test_norm)))

ggplot(tmp)+geom_line(aes(ind,GPP_NT_VUT_REF))+geom_line(aes(ind, pred),color="red")

```

```{r message=FALSE, warning=F}
tmp %>% 
  ggplot(aes(pred, GPP_NT_VUT_REF)) +
  geom_hex() +
  geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  scale_fill_gradientn(
    colours = colorRampPalette( c("gray65", "navy", "red", "yellow"))(5)) +
  xlim(-5, 20) + ylim(-5, 20) +
  theme_classic()
```

# LSTM in Flux Tutorial {#ch-14}

### Motivation

Machine learning and deep learning allows us to model the complex relationship between predictors and target variables. However, many popular models are time-agnostic such as random forest(RF) and fully-connected neural network(DNN). They assume the data comes in one timestamp, and we can shuffle the inputs without influencing the outputs. This assumption is not true in a lot of the studies. When we record the measurements in a range of time-span, we sometimes expect to see a trend depending on the collected time. The data naturally contains the time-dependency. Thus, it will be useful to have a model considering these temporal dependencies along with time-invariant metadata. In this tutorial, we will learn about LSTM+MLP to achieve this goal. 

We again use the FLUXNET dataset in this chapter. The measurements are at the daily resolution used together with the metadata from every site to predict `GPP_NT_VUT_REF`. We will use a new machine learning framework called `torch` developed based on `PyTorch`. It is a neural network library that computes the differentiation automatically and is powerful in GPU acceleration.  

### LSTM
Long Short-Term Memory neural network is a type of recurrent neural network. The inputs and outputs of LSTM are time-depended sequence. Compared to simple RNN model vanishing gradient problem is solved. Here is the details of the LSTM cell. For every time stamp we pass the data through such cell to compute the outputs. 

![lstm](graph/lstm_withsymbol.png)
$$i_t =  \sigma(W_{ii}x_t + b_{ii} +W_{hi}h(t-1) + b_{hi})$$
$$f_t = \sigma(W_{if}x_t + b_{if} +W_{hf}h(t-1) + b_{hf})$$

$$\tilde{C}_t = tanh(W_{ig}x_t + b_{ig}+W_{hg}h(t-1) + b_{hg})$$
$$o_t = \sigma(W{io}x_t + b_{io} +W_{ho}h(t-1) + b_{ho})$$
$$C_t = f_tC_{(t-1)} + i_tg_t$$
$$h_t = o_{t}tanh(c_t)$$
where $h_t$ is the hidden state at time t, $c_t$ is the cell state at time t, $x_t$ is the input at time t, $h_{(t-1)}$ is
the hidden state of the previous layer at time $t-1$ or the initial hidden state at time 0, and $i_t, f_t, \tilde{C}_t,
o_t$ are the input, forget, cell, and output gates, respectively. $\sigma$ is the sigmoid function.

### MLP
Multi-layer perception model is a type of artificial neural network(ANN). Every neural in a single layer is connected to all the neural in the next layer, so you can also see it is refereed as fully-connected neural network in some context. The edges are where we call the weights which are the important parameters to be learned in the training. Every layer contains the linear mapping and a non-linear "activation function", except for the output layer. The output layer applies a linear function to give rise to the output. 

![MLP](graph/mlp_diagram.png)
As LSTM only take care of the time-dependent values, we use LSTM+MLP to handle both type of data. The concatenated time invariant variables and the outputs of the hidden layer in LSTM used as the inputs of MLP. At the training step of such model, in the `forward` pass, we map inputs and the initial value of the weights to the outputs. In the `backward` pass we need to do two things to learn the parameter. One is computing the derivative with respect to the parameter by chain rule and the other is updating the values with chosen optimization function. As you can see later these two steps are corresponding to `loss$backward()` and `optimizer$step()` in torch. A commonly seen optimization function is gradient descent. 

$$\boldsymbol{w}_{n+1}=\boldsymbol{w}_{n}-\gamma \nabla f(\boldsymbol{w}_{n})$$
where $\boldsymbol{w}$ is the parameter, $\gamma$ is the learning rate and $\boldsymbol{w}_{n}$ is the derivative with respect to the parameter.

#### Example of apply the model on one site data
We first start with a toy example of using LSTM+MLP with the data from one site. Let's start with importing the needed packages.
```{r install packages,warning=F, message=F}
library(torch)
library(luz)
library(tidytable)
library(tidyverse)
library(skimr)
library(recipes)
```

Next, we import the dataset which including the daily measurements of one site and glimpse the summaries of the dataset.
```{r import data ,warning=F, message=F}
df<- read_csv("data/df_CH-Lae.csv")
skim(df)
```

Since the meta info of site does not change with time, we would like to keep temporal data and site metadata in separate files and handle data frames separately. 
```{r}
# define the meta variables and extract the site dataframe
meta_vars <- c("classid", "koeppen_code", "igbp_land_use", "plant_functional_type") 
df_sites <- df %>% 
  select(sitename, one_of(meta_vars)) %>% 
  distinct()

# define the measurement variables and extract the sensor dataframe
vars <- c("TA_F", "TA_F_DAY", "TA_F_NIGHT", "SW_IN_F", "LW_IN_F", "VPD_F", "PA_F", "P_F", "WS_F")
ddf <- df %>% 
  select(-one_of(meta_vars))
ddf <- df %>% 
  select(-"date")

# save the dataframe for further analysis 
write_csv(df_sites, file = "data/df_sites.csv")
write_csv(ddf, file = "data/ddf.csv")
```

```{r, message=F}
# import the data
df_sites <- read_csv("data/df_sites.csv")
ddf <- read_csv("data/ddf.csv")
```

For neural networks, we only pass the predictors through the model, thus in the next step we separate the target variable (GPP values) from the dataset. The GPP values will be used in the step of calculating the loss of the model to update/learn the parameters. 

```{r}
# Separate X and y variables
df_gpp <- df %>% select(GPP_NT_VUT_REF)
df_measure <- df %>% select(-GPP_NT_VUT_REF)
```

Similar to other machine learning task, we split the dataframe into the train and test sets. Note that since we are using a time dependency model, we need to obtain a time-continuous train set and test set. 

```{r}
idx_train <- seq(ceiling(0.7 * nrow(df)))
ddf_train <- ddf %>% 
  slice(idx_train)
ddf_test <- ddf %>% 
  slice(-idx_train)
```

```{r}
ggplot() +
  geom_line(data = ddf %>% mutate(idx = 1:nrow(.)), aes(idx, GPP_NT_VUT_REF)) +
  geom_vline(xintercept = max(idx_train), linetype = "dashed")+ggtitle("Visualising the train and test split")
```

We perform normalizing/centering/scaling with recipes package on training and testing data. 
```{r}
## define recipe
myrecipe_ddf <- recipe(
  GPP_NT_VUT_REF ~ TA_F + TA_F_DAY + TA_F_NIGHT + SW_IN_F + LW_IN_F + VPD_F + PA_F + P_F + WS_F + wscal + fpar,
  data = ddf_train) %>% 
  step_normalize(all_numeric(), id = "normalize")
  # step_center(all_numeric()) %>%
  # step_scale(all_numeric())

## determine normalisation parameters (mean, sd) on training data (only!)
pp <- prep(myrecipe_ddf, training = ddf_train)

## apply the same normalisation to training and testing
ddf_train_norm <- bake(pp, ddf_train)
ddf_test_norm <- bake(pp, ddf_test)

```

Here is the model LSTM+MLP Architecture. In `initialze` we define the objects that is needed for the model, including the function and the variables. We will use one LSTM followed by three fully connected layers. A final linear layer is necessary to map the last hidden layer to the outputs. The function `forward` is defining the architecture of the model that compute from predictors to outputs and how we pass the values through every layers. `in_features` and `out_features` are the number of edges attach to the node at that layer. Note that since in this case the metadata is the same for all the points, how to utilize the metadata in the model will be introduced in the next section.

```{r LSTM}

mymodule_generator <- nn_module(
  
  initialize = function(input_dim, 
                        # conditional_dim, 
                        hidden_dim, 
                        # conditional, 
                        num_layers = 1 # we have a default of 1 here
                        ){
    
    self$lstm <- nn_lstm(
      input_size = input_dim, 
      hidden_size = hidden_dim, 
      num_layers = num_layers
      )
    
    #self$conditional <- 0 # conditional
    
    # if (self$conditional == 1){
    #   self$fc1 <- nn_sequential(
    #     nn_linear(
    #       in_features = hidden_dim + conditional_dim, 
    #       out_features = 64
    #       ),
    #     nn_relu()
    #     ) 
    # } else {
    #   self$fc1 <- nn_sequential(
    #     nn_linear(
    #       in_features = hidden_dim, 
    #       out_features = 32), 
    #     nn_relu()
    #     )
    # }

    ## explain numbers and how they relate between layers    
    self$fc1 <- nn_sequential(nn_linear(in_features = hidden_dim, out_features = 32),
                              nn_relu()
                              )
    self$fc2 <- nn_sequential(nn_linear(in_features = 32, out_features = 32), 
                              nn_relu()
                              )    # previously in_features = 64 caused an error
    self$fc3 <- nn_sequential(nn_linear(in_features = 32, out_features = 16), 
                              nn_relu()
                              )
    self$fc4 <- nn_linear(16, 1)
    
  },
  
  forward = function(x){    
    
    out <- self$lstm(torch_stack(x, 1))#concatenate the sequence of tensors by row
    out <- out[[1]]$squeeze(1) #take the output in hidden state and squzze on the unused dimension
    
    # if (self$conditional == 1){
    #  out <- torch_cat(list(out,c), dim = 2) 
    # }
    
    y <- self$fc1(out)  
    y <- self$fc2(y)
    y <- self$fc3(y)
    y <- self$fc4(y)
    
  }
)
```

```{r test ,include=FALSE}
#input_dim <- 11
#hidden_dim <- 256

#lstm <- nn_lstm(input_size=input_dim, hidden_size=hidden_dim, num_layers=1)
#fc1 <- nn_sequential(nn_linear(in_features=hidden_dim, out_features=64), nn_relu())
#fc2 <- nn_sequential(nn_linear(in_features=64, out_features=32), nn_relu())
#fc3 <- nn_sequential(nn_linear(in_features=32, out_features=16), nn_relu())
#fc4 <- nn_linear(16, 1)
#x <- x_train
#dim(x)
#c=conditional_train
#dim(c)

#out <- lstm(torch_stack(x,1))
#length(out)

#dim(out[[1]])
#out = torch_cat(list(out[[1]]$squeeze(1),c), dim=2) 

#out[[2]]
#y <- fc1(out[[1]])
#x <- fc2(y)
#o <- fc3(x)
#p <- fc4(o)
#dim(p$squeeze(1))

```

We define the parameters for the model. The `INPUT_FEATURES` is number of predictors and we use here 256 hidden nodes (`HIDDEN_DIM`) in the output of the LSTM to MLP.

```{r}
# fixed seed
torch_manual_seed(40)

# define number of epoch we want to run
n_epochs <- 30

# the device we want to run the model
DEVICE <- if (cuda_is_available()){c("cuda")} else {c("cpu")}

# model parameters
INPUT_FEATURES <- ncol(ddf_train_norm %>% 
                         select(-GPP_NT_VUT_REF)
                       ) 
HIDDEN_DIM <- 256
# CONDITIONAL_FEATURES <- ncol(df_meta_train)
#conditional <- 0
```

We put the parameter together with model architecture and obtain the corresponding optimizer.

```{r}
# define model and optimizer
model <- mymodule_generator(
  INPUT_FEATURES,
  # CONDITIONAL_FEATURES, 
  HIDDEN_DIM,
  num_layers=1 
  )$to(device = DEVICE)

optimizer <- optim_adam(model$parameters, lr = 0.1)

#Prepare the function computing R2
rsq <- function(x, y){cor(x, y)^2}
```

Here is the training step.

```{r}
# training model with defined epochs
r2 <- c()
train_loss <- 0.0
train_r2 <- 0.0

for (epoch in 1:n_epochs){
  
    # put the model in train mode in which parameters are learnt in the backward pass
    model$train()
  
    # Since "torch" operates on the data type "tensor" and the function only accept R atomic vector. So we transform the dataframe to matrix then to tensor data type 
    x <- torch_tensor(ddf_train_norm %>% 
                        select(-GPP_NT_VUT_REF) %>% 
                        as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    
    y <- torch_tensor(ddf_train_norm %>% 
                        select(GPP_NT_VUT_REF) %>% 
                        as.matrix(), 
                      dtype = torch_float()
                      )$to(device = DEVICE)
    # c <- torch_tensor(conditional, 
    #                   dtype = torch_float()
    #                   )$to(device = DEVICE)
    
    y_pred <- model(x)
    optimizer$zero_grad()
    
    # compute the mse loss
    loss <- nnf_mse_loss(y_pred, y)
    
    # update the parameter
    loss$backward()
    optimizer$step()
    
    # compute the loss and R2 in the training
    train_loss <- train_loss + loss$item()
    train_r2 <- train_r2 + rsq(as.numeric(y), as.numeric(y_pred))

    # put the model in evaluation mode in which the parameters will not be updated.
    model$eval()
    
    # pass the test set into the model and compute the R2
    with_no_grad({
            x <- torch_tensor(ddf_test_norm %>% 
                                select(-GPP_NT_VUT_REF) %>% 
                                as.matrix(), # x_test, 
                              dtype = torch_float()
                              )#$cuda()
            y <- torch_tensor(ddf_test_norm %>% 
                                select(GPP_NT_VUT_REF) %>% 
                                as.matrix(), # y_test, 
                              dtype = torch_float()
                              )#$cuda()
            
#            c <- torch_tensor(conditional_test, 
#                              dtype = torch_float()
#                              )#$cuda()
            
            y_pred <- model(x)
            
            #compute the loss
            test_r2 <- rsq(as.numeric(y), as.numeric(y_pred))
            r2 <- c(r2, test_r2)
            
            #save the predictions 
            if (test_r2 >= max(r2)){cv_pred <- list(y_pred)}
            })
}

```

Printing the result with names of the site and the r2 we got.

```{r message=F}
library(R.utils)
# R2 for the model
printf(c("Site: %s\n", "R2: %s\n"), c(as.character(df$sitename[1]), max(r2)))
```

Visualise results.

```{r  ,warning=F,message=F}
cv_pred[[1]] %>% as.numeric() %>% hist()

mean <- tidy(pp, id = "normalize") %>%
  filter(terms == "GPP_NT_VUT_REF", statistic == "mean") %>%
  pull(value)

sd <- tidy(pp, id = "normalize") %>%
  filter(terms == "GPP_NT_VUT_REF", statistic == "sd") %>%
  pull(value)

ddf_train_norm %>% 
  mutate(GPP_NT_VUT_REF = GPP_NT_VUT_REF * sd + mean,
         pred = NA
         ) %>% 
  rbind(
    ddf_test_norm %>% 
      mutate(pred = cv_pred[[1]] %>% as.numeric()) %>% 
      mutate(GPP_NT_VUT_REF = GPP_NT_VUT_REF * sd + mean,
             pred = pred * sd + mean)
    ) %>% 
  mutate(idx = 1:nrow(.)) %>% 
  ggplot() +
  geom_line(aes(idx, GPP_NT_VUT_REF)) +
  geom_line(aes(idx, pred), color = "red") +
  geom_vline(xintercept = max(idx_train), linetype = "dashed")

```
